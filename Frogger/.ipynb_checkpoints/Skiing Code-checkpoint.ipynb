{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc4d674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (2.4.0) or chardet (4.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from collections import deque\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26ff8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame, shape=(84, 84)):\n",
    "    \"\"\"\n",
    "    Preprocess a single frame:\n",
    "    - Convert to grayscale\n",
    "    - Resize to desired shape\n",
    "    - Normalize pixel values\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Resize\n",
    "    resized = cv2.resize(gray, shape, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Normalize\n",
    "    normalized = resized.astype(np.float32) / 255.0\n",
    "\n",
    "    # Reshape to match model input\n",
    "    return normalized.reshape((*shape, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650c11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_replay_buffer(capacity):\n",
    "    \"\"\"Create a new experience replay buffer\"\"\"\n",
    "    return deque(maxlen=capacity)\n",
    "\n",
    "def add_to_buffer(buffer, state, action, reward, next_state, done):\n",
    "    \"\"\"Add transition to replay buffer\"\"\"\n",
    "    buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "def sample_from_buffer(buffer, batch_size):\n",
    "    \"\"\"Sample batch of transitions from replay buffer\"\"\"\n",
    "    batch = random.sample(buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c63a1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(state_shape, n_actions, learning_rate=0.00025):\n",
    "    \"\"\"Build CNN model for DQN\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=state_shape),\n",
    "        Conv2D(64, (4, 4), strides=(2, 2), activation='relu'),\n",
    "        Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(n_actions)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7981bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_model(model, target_model):\n",
    "    \"\"\"Copy weights from main model to target model\"\"\"\n",
    "    target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35a12164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, model, n_actions, epsilon):\n",
    "    \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(n_actions)\n",
    "\n",
    "    q_values = model.predict(state[np.newaxis, ...], verbose=0)\n",
    "    return np.argmax(q_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f150996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, target_model, replay_buffer, batch_size, gamma):\n",
    "    \"\"\"Train model using experience replay\"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return 0  # Return loss value (0 if no training happened)\n",
    "\n",
    "    # Sample batch from replay buffer\n",
    "    states, actions, rewards, next_states, dones = sample_from_buffer(replay_buffer, batch_size)\n",
    "\n",
    "    # Get target Q values from target model\n",
    "    target_q_values = target_model.predict(next_states, verbose=0)\n",
    "    max_target_q = np.max(target_q_values, axis=1)\n",
    "\n",
    "    # Calculate target using Bellman equation\n",
    "    targets = rewards + gamma * max_target_q * (1 - dones)\n",
    "\n",
    "    # Get current Q values and update with targets\n",
    "    q_values = model.predict(states, verbose=0)\n",
    "\n",
    "    # Update only the Q values for the actions taken\n",
    "    for i, action in enumerate(actions):\n",
    "        q_values[i][action] = targets[i]\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(states, q_values, epochs=1, verbose=0)\n",
    "    return history.history['loss'][0] if 'loss' in history.history else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "802c06eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_epsilon(epsilon, epsilon_min, epsilon_decay):\n",
    "    \"\"\"Decay exploration rate\"\"\"\n",
    "    if epsilon > epsilon_min:\n",
    "        return epsilon * epsilon_decay\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53f32663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, target_model, filepath):\n",
    "    \"\"\"Load model weights from file\"\"\"\n",
    "    model.load_weights(filepath)\n",
    "    update_target_model(model, target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8a5b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_weights(model, filepath):\n",
    "    \"\"\"Save model weights to file\"\"\"\n",
    "    model.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a7369cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skiing_agent(num_episodes=50000, target_update_freq=1000,\n",
    "                      save_freq=100, render=False, model_path=\"skiing_dqn_model.h5\"):\n",
    "    \"\"\"Train DQN agent on Skiing environment\"\"\"\n",
    "    # Create Atari environment\n",
    "    env = gym.make(\"ALE/Skiing-v5\")\n",
    "\n",
    "    # Define parameters\n",
    "    processed_frame_shape = (84, 84, 1)  # Single frame, not stacked\n",
    "    n_actions = env.action_space.n\n",
    "    learning_rate = 0.00025\n",
    "    gamma = 0.99  # Discount factor\n",
    "    epsilon = 1.0  # Initial exploration rate\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay = 0.995\n",
    "    buffer_size = 100000\n",
    "    batch_size = 32\n",
    "\n",
    "    # Create models and replay buffer\n",
    "    model = build_model(processed_frame_shape, n_actions, learning_rate)\n",
    "    target_model = build_model(processed_frame_shape, n_actions, learning_rate)\n",
    "    update_target_model(model, target_model)\n",
    "    replay_buffer = create_replay_buffer(buffer_size)\n",
    "\n",
    "    # Training statistics\n",
    "    episode_rewards = []\n",
    "    step_counter = 0\n",
    "\n",
    "    # Main training loop\n",
    "    for episode in range(num_episodes):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Reset environment and preprocess initial frame\n",
    "        observation, info = env.reset()\n",
    "        state = process_frame(observation)\n",
    "\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        step = 0\n",
    "\n",
    "        # Episode loop\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Choose and perform action\n",
    "            action = choose_action(state, model, n_actions, epsilon)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Process new frame\n",
    "            next_state = process_frame(observation)\n",
    "\n",
    "            # Store transition\n",
    "            add_to_buffer(replay_buffer, state, action, reward, next_state, done)\n",
    "\n",
    "            # Train network\n",
    "            train_model(model, target_model, replay_buffer, batch_size, gamma)\n",
    "\n",
    "            # Update state and counters\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "            step_counter += 1\n",
    "\n",
    "            # Update target network periodically\n",
    "            if step_counter % target_update_freq == 0:\n",
    "                update_target_model(model, target_model)\n",
    "\n",
    "        # Decay epsilon after each episode\n",
    "        epsilon = decay_epsilon(epsilon, epsilon_min, epsilon_decay)\n",
    "\n",
    "        # End of episode processing\n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "        # Calculate average reward over last 100 episodes\n",
    "        avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "\n",
    "        # Display progress\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Episode: {episode+1}/{num_episodes}, Steps: {step}, Reward: {episode_reward:.2f}, \"\n",
    "              f\"Avg Reward: {avg_reward:.2f}, Epsilon: {epsilon:.4f}, Duration: {duration:.2f}s\")\n",
    "\n",
    "        # Save model periodically\n",
    "        if (episode + 1) % save_freq == 0:\n",
    "            save_model_weights(model, f\"{model_path}_episode_{episode+1}\")\n",
    "            print(f\"Model saved at episode {episode+1}\")\n",
    "\n",
    "    # Final save\n",
    "    save_model_weights(model, model_path)\n",
    "    env.close()\n",
    "\n",
    "    return model, target_model, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c0229b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10612\\12821840.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mnum_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_EPISODES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mrender\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRENDER_TRAINING\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m  )\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training and testing completed.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MODEL_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# Train agent\n",
    "NUM_EPISODES = 10000\n",
    "RENDER_TRAINING = False\n",
    "print(\"Starting training...\")\n",
    "model, target_model, rewards = train_skiing_agent(\n",
    "        num_episodes=NUM_EPISODES,\n",
    "        render=RENDER_TRAINING,\n",
    "        model_path=MODEL_PATH\n",
    " )\n",
    "print(\"Training and testing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(model_path, num_episodes):\n",
    "    \"\"\"Test trained DQN agent on Skiing environment\"\"\"\n",
    "    \n",
    "    # Define parameters\n",
    "    processed_frame_shape = (84, 84, 1)  # Match training shape\n",
    "    n_actions = env.action_space.n\n",
    "    epsilon_test = 0.01  # Small epsilon for some exploration during testing\n",
    "\n",
    "    # Create models\n",
    "    model = build_model(processed_frame_shape, n_actions)\n",
    "    target_model = build_model(processed_frame_shape, n_actions)\n",
    "    \n",
    "    # Load trained model\n",
    "    load_model_weights(model, target_model, model_path)\n",
    "\n",
    "    # Test loop\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset environment\n",
    "        observation, info = env.reset()\n",
    "        state = process_frame(observation)  # Single frame processing\n",
    "\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        step = 0\n",
    "\n",
    "        # Episode loop\n",
    "        while not done:\n",
    "\n",
    "            # Choose action\n",
    "            action = choose_action(state, model, n_actions, epsilon_test)\n",
    "            observation, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Process new frame\n",
    "            next_state = process_frame(observation)  # Single frame processing\n",
    "\n",
    "            # Update state and counters\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "\n",
    "        print(f\"Test Episode: {episode+1}/{num_episodes}, Steps: {step}, Reward: {episode_reward:.2f}\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test agent\n",
    "env = gym.make(\"ALE/Skiing-v5\", render_mode=\"human\")\n",
    "obsersvation, info = env.reset()\n",
    "MODEL_PATH = \"skiing_dqn_model.h5_episode_10.weights.h5\"\n",
    "test_agent(MODEL_PATH, num_episodes=10)  # Test for 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d496d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
