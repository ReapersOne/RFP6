{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d813817f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install \"numpy<2.0.0\"\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!pip install \"numpy<2.0.0\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5351f260",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (2.4.0) or chardet (4.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import envs\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import time\n",
    "import warnings\n",
    "import ale_py\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6881d38",
   "metadata": {},
   "source": [
    "# Please experiment with the numbers, we want to make it to the end, and I know it'll take a while so please do not randomly choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2e3797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wreep\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\wreep\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded weights from frogger_weights_ep2700.weights.h5\n",
      "Episode: 1/3000, Reward: 9.0, Epsilon: 0.50, Time Elapsed: 0.42 mins, Remaining: 1267.89 mins\n",
      "Episode: 2/3000, Reward: 12.0, Epsilon: 0.50, Time Elapsed: 0.84 mins, Remaining: 1263.06 mins\n",
      "Episode: 3/3000, Reward: 19.0, Epsilon: 0.49, Time Elapsed: 1.53 mins, Remaining: 1533.15 mins\n",
      "Episode: 4/3000, Reward: 13.0, Epsilon: 0.49, Time Elapsed: 2.12 mins, Remaining: 1587.77 mins\n",
      "Episode: 5/3000, Reward: 17.0, Epsilon: 0.49, Time Elapsed: 2.66 mins, Remaining: 1594.31 mins\n",
      "Episode: 6/3000, Reward: 12.0, Epsilon: 0.49, Time Elapsed: 3.10 mins, Remaining: 1547.46 mins\n",
      "Episode: 7/3000, Reward: 18.0, Epsilon: 0.48, Time Elapsed: 3.57 mins, Remaining: 1526.05 mins\n",
      "Episode: 8/3000, Reward: 14.0, Epsilon: 0.48, Time Elapsed: 4.00 mins, Remaining: 1495.28 mins\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3152\\1367417944.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[0mweights_to_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"frogger_weights_ep2700.weights.h5\"\u001b[0m  \u001b[1;31m# Change this to your weights file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights_to_load\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"frogger_final_weights_enhanced.weights.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3152\\1367417944.py\u001b[0m in \u001b[0;36mtrain_dqn\u001b[1;34m(weights_path)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\trainers\\epoch_iterator.py\u001b[0m in \u001b[0;36m_enumerate_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_seen\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_seen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    707\u001b[0m             \u001b[1;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m             \"not be specified.\")\n\u001b[1;32m--> 709\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    746\u001b[0m             self._flat_output_types)\n\u001b[0;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3476\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3477\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3478\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   3479\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0;32m   3480\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Hyperparameters ---\n",
    "EPISODES = 3000                \n",
    "BATCH_SIZE = 32                \n",
    "MEMORY_SIZE = 100000           \n",
    "GAMMA = 0.99                   \n",
    "EPSILON_START = 0.5            \n",
    "EPSILON_MIN = 0.01             \n",
    "EPSILON_DECAY = 0.995          \n",
    "LEARNING_RATE = 0.00025        \n",
    "UPDATE_TARGET_FREQ = 1000      \n",
    "FRAME_STACK = 4                \n",
    "SAVE_FREQ = 50                 \n",
    "\n",
    "# --- Prioritized Experience Replay (PER) ---\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.priorities = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        max_priority = max(self.priorities) if self.buffer else 1.0\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(max_priority)\n",
    "    \n",
    "    def sample(self, batch_size, alpha=0.6):\n",
    "        priorities = np.array(self.priorities)\n",
    "        probs = priorities ** alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-0.4)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        states = np.array([x[0] for x in samples])\n",
    "        actions = np.array([x[1] for x in samples])\n",
    "        rewards = np.array([x[2] for x in samples])\n",
    "        next_states = np.array([x[3] for x in samples])\n",
    "        dones = np.array([x[4] for x in samples])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, errors, offset=0.01):\n",
    "        for i, error in zip(indices, errors):\n",
    "            self.priorities[i] = abs(error) + offset\n",
    "\n",
    "# --- Environment Setup ---\n",
    "env = gym.make(\"ALE/Frogger-v5\", render_mode=\"rgb_array\")\n",
    "state_shape = (84, 84, FRAME_STACK)\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# --- Model Definition ---\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (8, 8), strides=4, activation=\"relu\", input_shape=state_shape),\n",
    "        Conv2D(64, (4, 4), strides=2, activation=\"relu\"),\n",
    "        Conv2D(64, (3, 3), strides=1, activation=\"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dense(action_size, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "    return model\n",
    "\n",
    "# --- Frame Stacking ---\n",
    "class FrameStacker:\n",
    "    def __init__(self):\n",
    "        self.frames = deque(maxlen=FRAME_STACK)\n",
    "    \n",
    "    def reset(self, state):\n",
    "        for _ in range(FRAME_STACK):\n",
    "            self.frames.append(state)\n",
    "        return np.stack(self.frames, axis=-1)\n",
    "    \n",
    "    def append(self, state):\n",
    "        self.frames.append(state)\n",
    "        return np.stack(self.frames, axis=-1)\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess_state(state):\n",
    "    state = np.mean(state, axis=2)  \n",
    "    state = state[34:194, :]        \n",
    "    state = state[::2, ::2]         \n",
    "    state = np.pad(state, ((2,2),(2,2)), mode='constant')\n",
    "    return state / 255.0\n",
    "\n",
    "# --- Training Functions ---\n",
    "def load_model_weights(model, weights_path):\n",
    "    if weights_path:\n",
    "        try:\n",
    "            model.load_weights(weights_path)\n",
    "            print(f\"Successfully loaded weights from {weights_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load weights from {weights_path}: {str(e)}\")\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "def train_dqn(weights_path=None):  # Add weights_path parameter\n",
    "    model = build_model()\n",
    "    target_model = clone_model(model)\n",
    "    \n",
    "    # Load weights if provided\n",
    "    if weights_path:\n",
    "        try:\n",
    "            model.load_weights(weights_path)\n",
    "            print(f\"Successfully loaded weights from {weights_path}\")\n",
    "        except:\n",
    "            print(f\"Could not load weights from {weights_path}, starting from scratch\")\n",
    "    \n",
    "    target_model.set_weights(model.get_weights())\n",
    "    \n",
    "    memory = PrioritizedReplayBuffer(MEMORY_SIZE)  \n",
    "    frame_stacker = FrameStacker()\n",
    "    epsilon = EPSILON_START\n",
    "    rewards_history = []\n",
    "    start_time = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for episode in range(1, EPISODES + 1):\n",
    "        state, _ = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "        state = frame_stacker.reset(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            global_step += 1\n",
    "\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "                action = np.argmax(q_values[0])\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = preprocess_state(next_state)\n",
    "            next_state = frame_stacker.append(next_state)\n",
    "            memory.add(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if global_step % 4 == 0 and len(memory.buffer) >= BATCH_SIZE:\n",
    "                states, actions, rewards, next_states, dones, indices, weights = memory.sample(BATCH_SIZE)\n",
    "                \n",
    "                next_q_values = target_model.predict(next_states, verbose=0)\n",
    "                best_actions = np.argmax(model.predict(next_states, verbose=0), axis=1)\n",
    "                target_q = rewards + GAMMA * next_q_values[np.arange(BATCH_SIZE), best_actions] * (1 - dones)\n",
    "                \n",
    "                current_q = model.predict(states, verbose=0)\n",
    "                td_errors = target_q - current_q[np.arange(BATCH_SIZE), actions]\n",
    "                memory.update_priorities(indices, td_errors)\n",
    "                \n",
    "                target = current_q.copy()\n",
    "                target[np.arange(BATCH_SIZE), actions] = target_q\n",
    "                model.fit(states, target, sample_weight=weights, epochs=1, verbose=0)\n",
    "\n",
    "            if global_step % UPDATE_TARGET_FREQ == 0:\n",
    "                target_model.set_weights(model.get_weights())\n",
    "\n",
    "        if epsilon > EPSILON_MIN:\n",
    "            epsilon *= EPSILON_DECAY\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "\n",
    "        elapsed_time = (time.time() - start_time) / 60  # in minutes\n",
    "        remaining_time = (elapsed_time / episode) * (EPISODES - episode)\n",
    "        print(f\"Episode: {episode}/{EPISODES}, Reward: {total_reward}, Epsilon: {epsilon:.2f}, Time Elapsed: {elapsed_time:.2f} mins, Remaining: {remaining_time:.2f} mins\")\n",
    "\n",
    "        if episode % SAVE_FREQ == 0:\n",
    "            model.save_weights(f\"frogger_weights_ep{episode}.weights.h5\")\n",
    "            print(f\"Saved weights at episode {episode}\")\n",
    "\n",
    "    plt.plot(rewards_history)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Frogger DQN Training (PER + Double DQN + Frame Stack)\")\n",
    "    plt.savefig(\"frogger_training_enhanced.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Run Training ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the path to your existing weights file (or None to start fresh)\n",
    "    weights_to_load = \"frogger_weights_ep2700.weights.h5\"  # Change this to your weights file\n",
    "    \n",
    "    model = train_dqn(weights_path=weights_to_load)\n",
    "    model.save_weights(\"frogger_final_weights_enhanced.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32749ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
