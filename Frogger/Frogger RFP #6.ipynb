{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8677f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "957fa03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import envs\n",
    "\n",
    "# List all registered environments (including Atari)\n",
    "all_envs = envs.registry.keys()\n",
    "atari_envs = [env_id for env_id in all_envs if \"ALE/\" in env_id]\n",
    "\n",
    "# Print first 10 Atari games\n",
    "print(list(atari_envs)[:10])  # Should include 'ALE/Frogger-v5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5351f260",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wreep\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\wreep\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import warnings\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72332e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport gymnasium as gym\\n\\n# Modern way to create Atari environment\\nenv = gym.make(\"ALE/Frogger-v5\", render_mode=\"human\")  # Note: ALE/ prefix required\\n\\n\\nobservation, info = env.reset()\\nfor _ in range(100):\\n    action = env.action_space.sample()  # Random action\\n    observation, reward, terminated, truncated, info = env.step(action)\\n        \\n    if terminated or truncated:\\n        observation, info = env.reset()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import gymnasium as gym\n",
    "\n",
    "# Modern way to create Atari environment\n",
    "env = gym.make(\"ALE/Frogger-v5\", render_mode=\"human\")  # Note: ALE/ prefix required\n",
    "\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b91a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define the environment runner function\n",
    "def run_frogger_instance(instance_id, num_episodes=5, render_mode=None):\n",
    "    \"\"\"\n",
    "    Runs a single instance of the Frogger environment\n",
    "    \n",
    "    Args:\n",
    "        instance_id (int): Identifier for this process\n",
    "        num_episodes (int): Number of episodes to run\n",
    "        render_mode (str): None, \"human\", or \"rgb_array\"\n",
    "    \"\"\"\n",
    "    env = gym.make(\"ALE/Frogger-v5\", render_mode=render_mode)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Replace this with your actual policy/controller\n",
    "            action = env.action_space.sample()  # Random actions for demo\n",
    "            \n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Add your custom processing here\n",
    "            \n",
    "        print(f\"Instance {instance_id} | Episode {episode+1}/{num_episodes} | Reward: {total_reward}\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3ce2dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 4 parallel Frogger instances at 2025-04-09 10:04:09\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Main execution block (must be in its own cell)\n",
    "if __name__ == \"__main__\":  # Critical for multiprocessing in Jupyter\n",
    "    # Configuration\n",
    "    NUM_INSTANCES = 4       # Number of parallel instances\n",
    "    EPISODES_PER_INSTANCE = 3\n",
    "    RENDER_MODE = None      # Set to \"human\" if you want visualization\n",
    "    \n",
    "    print(f\"Starting {NUM_INSTANCES} parallel Frogger instances at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "384e3be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All instances completed in 0.08 seconds\n",
      "All instances completed in 0.15 seconds\n",
      "All instances completed in 0.21 seconds\n",
      "All instances completed in 0.28 seconds\n"
     ]
    }
   ],
   "source": [
    "   # Create and start processes\n",
    "processes = []\n",
    "for i in range(NUM_INSTANCES):\n",
    "        # Only render the first instance if RENDER_MODE is \"human\"\n",
    "    current_render_mode = RENDER_MODE if i == 0 else None\n",
    "        \n",
    "    p = mp.Process(\n",
    "        target=run_frogger_instance,\n",
    "        args=(i, EPISODES_PER_INSTANCE, current_render_mode)\n",
    "        )\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "    \n",
    "    # Wait for all processes to complete\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    \n",
    "    duration = (datetime.now() - start_time).total_seconds()\n",
    "    print(f\"All instances completed in {duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6017b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "def run_env(env_name, num_episodes, process_id):\n",
    "    env = gym.make(env_name, render_mode=\"human\")  # or \"rgb_array\" for no rendering\n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Random policy - replace with your agent\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            # Add your learning/processing here\n",
    "        print(f\"Process {process_id} finished episode {episode}\")\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = \"ALE/Frogger-v5\"  # Or the correct Frogger environment name\n",
    "    num_processes = 4  # Number of parallel instances\n",
    "    num_episodes = 10  # Episodes per instance\n",
    "    \n",
    "    processes = []\n",
    "    for i in range(num_processes):\n",
    "        p = mp.Process(target=run_env, args=(env_name, num_episodes, i))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e3797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\wreep\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/1000, Reward: 13.0, Epsilon: 0.99, Time Elapsed: 0.24 mins, Remaining: 241.80 mins\n",
      "Episode: 2/1000, Reward: 10.0, Epsilon: 0.99, Time Elapsed: 0.46 mins, Remaining: 228.18 mins\n",
      "Episode: 3/1000, Reward: 8.0, Epsilon: 0.99, Time Elapsed: 0.64 mins, Remaining: 213.20 mins\n",
      "Episode: 4/1000, Reward: 11.0, Epsilon: 0.98, Time Elapsed: 0.89 mins, Remaining: 220.39 mins\n",
      "Episode: 5/1000, Reward: 8.0, Epsilon: 0.98, Time Elapsed: 1.06 mins, Remaining: 210.27 mins\n",
      "Episode: 6/1000, Reward: 12.0, Epsilon: 0.97, Time Elapsed: 1.29 mins, Remaining: 214.23 mins\n",
      "Episode: 7/1000, Reward: 7.0, Epsilon: 0.97, Time Elapsed: 1.51 mins, Remaining: 213.96 mins\n",
      "Episode: 8/1000, Reward: 8.0, Epsilon: 0.96, Time Elapsed: 1.70 mins, Remaining: 211.25 mins\n",
      "Episode: 9/1000, Reward: 8.0, Epsilon: 0.96, Time Elapsed: 1.88 mins, Remaining: 207.37 mins\n",
      "Episode: 10/1000, Reward: 10.0, Epsilon: 0.95, Time Elapsed: 2.09 mins, Remaining: 206.66 mins\n",
      "Episode: 11/1000, Reward: 6.0, Epsilon: 0.95, Time Elapsed: 2.30 mins, Remaining: 206.76 mins\n",
      "Episode: 12/1000, Reward: 10.0, Epsilon: 0.94, Time Elapsed: 2.60 mins, Remaining: 213.69 mins\n",
      "Episode: 13/1000, Reward: 10.0, Epsilon: 0.94, Time Elapsed: 2.79 mins, Remaining: 211.48 mins\n",
      "Episode: 14/1000, Reward: 9.0, Epsilon: 0.93, Time Elapsed: 3.02 mins, Remaining: 212.65 mins\n",
      "Episode: 15/1000, Reward: 10.0, Epsilon: 0.93, Time Elapsed: 3.23 mins, Remaining: 211.86 mins\n",
      "Episode: 16/1000, Reward: 10.0, Epsilon: 0.92, Time Elapsed: 3.49 mins, Remaining: 214.35 mins\n",
      "Episode: 17/1000, Reward: 8.0, Epsilon: 0.92, Time Elapsed: 3.69 mins, Remaining: 213.15 mins\n",
      "Episode: 18/1000, Reward: 8.0, Epsilon: 0.91, Time Elapsed: 3.89 mins, Remaining: 212.45 mins\n",
      "Episode: 19/1000, Reward: 11.0, Epsilon: 0.91, Time Elapsed: 4.14 mins, Remaining: 213.97 mins\n",
      "Episode: 20/1000, Reward: 11.0, Epsilon: 0.90, Time Elapsed: 4.40 mins, Remaining: 215.36 mins\n",
      "Episode: 21/1000, Reward: 13.0, Epsilon: 0.90, Time Elapsed: 4.66 mins, Remaining: 217.45 mins\n",
      "Episode: 22/1000, Reward: 12.0, Epsilon: 0.90, Time Elapsed: 4.89 mins, Remaining: 217.58 mins\n",
      "Episode: 23/1000, Reward: 11.0, Epsilon: 0.89, Time Elapsed: 5.17 mins, Remaining: 219.65 mins\n",
      "Episode: 24/1000, Reward: 8.0, Epsilon: 0.89, Time Elapsed: 5.42 mins, Remaining: 220.25 mins\n",
      "Episode: 25/1000, Reward: 17.0, Epsilon: 0.88, Time Elapsed: 5.69 mins, Remaining: 222.00 mins\n",
      "Episode: 26/1000, Reward: 11.0, Epsilon: 0.88, Time Elapsed: 5.94 mins, Remaining: 222.37 mins\n",
      "Episode: 27/1000, Reward: 7.0, Epsilon: 0.87, Time Elapsed: 6.20 mins, Remaining: 223.34 mins\n",
      "Episode: 28/1000, Reward: 10.0, Epsilon: 0.87, Time Elapsed: 6.45 mins, Remaining: 224.06 mins\n",
      "Episode: 29/1000, Reward: 9.0, Epsilon: 0.86, Time Elapsed: 6.67 mins, Remaining: 223.31 mins\n",
      "Episode: 30/1000, Reward: 8.0, Epsilon: 0.86, Time Elapsed: 6.90 mins, Remaining: 223.24 mins\n",
      "Episode: 31/1000, Reward: 10.0, Epsilon: 0.86, Time Elapsed: 7.14 mins, Remaining: 223.31 mins\n",
      "Episode: 32/1000, Reward: 11.0, Epsilon: 0.85, Time Elapsed: 7.39 mins, Remaining: 223.51 mins\n",
      "Episode: 33/1000, Reward: 11.0, Epsilon: 0.85, Time Elapsed: 7.65 mins, Remaining: 224.22 mins\n",
      "Episode: 34/1000, Reward: 10.0, Epsilon: 0.84, Time Elapsed: 7.90 mins, Remaining: 224.55 mins\n",
      "Episode: 35/1000, Reward: 10.0, Epsilon: 0.84, Time Elapsed: 8.15 mins, Remaining: 224.68 mins\n",
      "Episode: 36/1000, Reward: 9.0, Epsilon: 0.83, Time Elapsed: 8.38 mins, Remaining: 224.32 mins\n",
      "Episode: 37/1000, Reward: 8.0, Epsilon: 0.83, Time Elapsed: 8.65 mins, Remaining: 225.14 mins\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import math\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "EPISODES = 1000                # Total training episodes\n",
    "BATCH_SIZE = 32                # Batch size for training\n",
    "MEMORY_SIZE = 100000           # Larger replay memory (for PER)\n",
    "GAMMA = 0.99                   # Higher discount factor\n",
    "EPSILON_START = 1.0            # Starting exploration rate\n",
    "EPSILON_MIN = 0.01             # Min exploration rate\n",
    "EPSILON_DECAY = 0.995          # Decay rate\n",
    "LEARNING_RATE = 0.00025        # Lower learning rate (better stability)\n",
    "UPDATE_TARGET_FREQ = 1000      # Double DQN target network update\n",
    "FRAME_STACK = 4                # Number of stacked frames (temporal info)\n",
    "SAVE_FREQ = 50                 # Save weights every N episodes\n",
    "\n",
    "# --- Prioritized Experience Replay (PER) ---\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.priorities = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        max_priority = max(self.priorities) if self.buffer else 1.0\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(max_priority)\n",
    "    \n",
    "    def sample(self, batch_size, alpha=0.6):\n",
    "        priorities = np.array(self.priorities)\n",
    "        probs = priorities ** alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-0.4)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        states = np.array([x[0] for x in samples])\n",
    "        actions = np.array([x[1] for x in samples])\n",
    "        rewards = np.array([x[2] for x in samples])\n",
    "        next_states = np.array([x[3] for x in samples])\n",
    "        dones = np.array([x[4] for x in samples])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, errors, offset=0.01):\n",
    "        for i, error in zip(indices, errors):\n",
    "            self.priorities[i] = abs(error) + offset\n",
    "\n",
    "# --- Environment Setup ---\n",
    "env = gym.make(\"ALE/Frogger-v5\", render_mode=\"rgb_array\")\n",
    "state_shape = (84, 84, FRAME_STACK)  # Stacked frames\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# --- Model Definition ---\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (8, 8), strides=4, activation=\"relu\", input_shape=state_shape),\n",
    "        Conv2D(64, (4, 4), strides=2, activation=\"relu\"),\n",
    "        Conv2D(64, (3, 3), strides=1, activation=\"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dense(action_size, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "    return model\n",
    "\n",
    "# --- Frame Stacking ---\n",
    "class FrameStacker:\n",
    "    def __init__(self):\n",
    "        self.frames = deque(maxlen=FRAME_STACK)\n",
    "    \n",
    "    def reset(self, state):\n",
    "        for _ in range(FRAME_STACK):\n",
    "            self.frames.append(state)\n",
    "        return np.stack(self.frames, axis=-1)\n",
    "    \n",
    "    def append(self, state):\n",
    "        self.frames.append(state)\n",
    "        return np.stack(self.frames, axis=-1)\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess_state(state):\n",
    "    state = np.mean(state, axis=2)  # Grayscale\n",
    "    state = state[34:194, :]        # Crop\n",
    "    state = state[::2, ::2]         # Downsample to 80x80\n",
    "    # Add padding to make it 84x84 if needed\n",
    "    state = np.pad(state, ((2,2),(2,2)), mode='constant')\n",
    "    return state / 255.0\n",
    "\n",
    "# --- Training Functions ---\n",
    "def train_dqn():\n",
    "    model = build_model()\n",
    "    target_model = clone_model(model)  # Double DQN\n",
    "    target_model.set_weights(model.get_weights())\n",
    "    \n",
    "    memory = PrioritizedReplayBuffer(MEMORY_SIZE)  # PER\n",
    "    frame_stacker = FrameStacker()\n",
    "    epsilon = EPSILON_START\n",
    "    rewards_history = []\n",
    "    start_time = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for episode in range(1, EPISODES + 1):\n",
    "        state, _ = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "        state = frame_stacker.reset(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            global_step += 1\n",
    "\n",
    "            # Epsilon-greedy action\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "                action = np.argmax(q_values[0])\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = preprocess_state(next_state)\n",
    "            next_state = frame_stacker.append(next_state)\n",
    "            memory.add(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Train on replay memory (PER)\n",
    "            if global_step % 4 == 0 and len(memory.buffer) >= BATCH_SIZE:\n",
    "                states, actions, rewards, next_states, dones, indices, weights = memory.sample(BATCH_SIZE)\n",
    "                \n",
    "                # Double DQN: Use target model for next Q-values\n",
    "                next_q_values = target_model.predict(next_states, verbose=0)\n",
    "                best_actions = np.argmax(model.predict(next_states, verbose=0), axis=1)\n",
    "                target_q = rewards + GAMMA * next_q_values[np.arange(BATCH_SIZE), best_actions] * (1 - dones)\n",
    "                \n",
    "                # Compute TD errors and update priorities\n",
    "                current_q = model.predict(states, verbose=0)\n",
    "                td_errors = target_q - current_q[np.arange(BATCH_SIZE), actions]\n",
    "                memory.update_priorities(indices, td_errors)\n",
    "                \n",
    "                # Train with importance-sampling weights\n",
    "                target = current_q.copy()\n",
    "                target[np.arange(BATCH_SIZE), actions] = target_q\n",
    "                model.fit(states, target, sample_weight=weights, epochs=1, verbose=0)\n",
    "\n",
    "            # Update target network (Double DQN)\n",
    "            if global_step % UPDATE_TARGET_FREQ == 0:\n",
    "                target_model.set_weights(model.get_weights())\n",
    "\n",
    "        # Decay epsilon\n",
    "        if epsilon > EPSILON_MIN:\n",
    "            epsilon *= EPSILON_DECAY\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "\n",
    "        # Print progress\n",
    "        elapsed_time = (time.time() - start_time) / 60  # in minutes\n",
    "        remaining_time = (elapsed_time / episode) * (EPISODES - episode)\n",
    "        print(f\"Episode: {episode}/{EPISODES}, Reward: {total_reward}, Epsilon: {epsilon:.2f}, Time Elapsed: {elapsed_time:.2f} mins, Remaining: {remaining_time:.2f} mins\")\n",
    "\n",
    "        # Save weights periodically\n",
    "        if episode % SAVE_FREQ == 0:\n",
    "            model.save_weights(f\"frogger_weights_ep{episode}.weights.h5\")\n",
    "            print(f\"Saved weights at episode {episode}\")\n",
    "\n",
    "    # Plot rewards\n",
    "    plt.plot(rewards_history)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Frogger DQN Training (PER + Double DQN + Frame Stack)\")\n",
    "    plt.savefig(\"frogger_training_enhanced.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Run Training ---\n",
    "if __name__ == \"__main__\":\n",
    "    model = train_dqn()\n",
    "    model.save_weights(\"frogger_final_weights_enhanced.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84dbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
