{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a3c0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "957fa03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import envs\n",
    "\n",
    "# List all registered environments (including Atari)\n",
    "all_envs = envs.registry.keys()\n",
    "atari_envs = [env_id for env_id in all_envs if \"ALE/\" in env_id]\n",
    "\n",
    "# Print first 10 Atari games\n",
    "print(list(atari_envs)[:10])  # Should include 'ALE/Frogger-v5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5351f260",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wreep\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\wreep\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import warnings\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72332e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport gymnasium as gym\\n\\n# Modern way to create Atari environment\\nenv = gym.make(\"ALE/Frogger-v5\", render_mode=\"human\")  # Note: ALE/ prefix required\\n\\n\\nobservation, info = env.reset()\\nfor _ in range(100):\\n    action = env.action_space.sample()  # Random action\\n    observation, reward, terminated, truncated, info = env.step(action)\\n        \\n    if terminated or truncated:\\n        observation, info = env.reset()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import gymnasium as gym\n",
    "\n",
    "# Modern way to create Atari environment\n",
    "env = gym.make(\"ALE/Frogger-v5\", render_mode=\"human\")  # Note: ALE/ prefix required\n",
    "\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec36abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define the environment runner function\n",
    "def run_frogger_instance(instance_id, num_episodes=5, render_mode=None):\n",
    "    \"\"\"\n",
    "    Runs a single instance of the Frogger environment\n",
    "    \n",
    "    Args:\n",
    "        instance_id (int): Identifier for this process\n",
    "        num_episodes (int): Number of episodes to run\n",
    "        render_mode (str): None, \"human\", or \"rgb_array\"\n",
    "    \"\"\"\n",
    "    env = gym.make(\"ALE/Frogger-v5\", render_mode=render_mode)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Replace this with your actual policy/controller\n",
    "            action = env.action_space.sample()  # Random actions for demo\n",
    "            \n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Add your custom processing here\n",
    "            \n",
    "        print(f\"Instance {instance_id} | Episode {episode+1}/{num_episodes} | Reward: {total_reward}\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "537e4160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 4 parallel Frogger instances at 2025-04-09 09:23:57\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Main execution block (must be in its own cell)\n",
    "if __name__ == \"__main__\":  # Critical for multiprocessing in Jupyter\n",
    "    # Configuration\n",
    "    NUM_INSTANCES = 4       # Number of parallel instances\n",
    "    EPISODES_PER_INSTANCE = 3\n",
    "    RENDER_MODE = None      # Set to \"human\" if you want visualization\n",
    "    \n",
    "    print(f\"Starting {NUM_INSTANCES} parallel Frogger instances at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3aa13be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All instances completed in 0.07 seconds\n",
      "All instances completed in 0.13 seconds\n",
      "All instances completed in 0.19 seconds\n",
      "All instances completed in 0.25 seconds\n"
     ]
    }
   ],
   "source": [
    "   # Create and start processes\n",
    "processes = []\n",
    "for i in range(NUM_INSTANCES):\n",
    "        # Only render the first instance if RENDER_MODE is \"human\"\n",
    "    current_render_mode = RENDER_MODE if i == 0 else None\n",
    "        \n",
    "    p = mp.Process(\n",
    "        target=run_frogger_instance,\n",
    "        args=(i, EPISODES_PER_INSTANCE, current_render_mode)\n",
    "        )\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "    \n",
    "    # Wait for all processes to complete\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    \n",
    "    duration = (datetime.now() - start_time).total_seconds()\n",
    "    print(f\"All instances completed in {duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad554c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "def run_env(env_name, num_episodes, process_id):\n",
    "    env = gym.make(env_name, render_mode=\"human\")  # or \"rgb_array\" for no rendering\n",
    "    for episode in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Random policy - replace with your agent\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            # Add your learning/processing here\n",
    "        print(f\"Process {process_id} finished episode {episode}\")\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = \"ALE/Frogger-v5\"  # Or the correct Frogger environment name\n",
    "    num_processes = 4  # Number of parallel instances\n",
    "    num_episodes = 10  # Episodes per instance\n",
    "    \n",
    "    processes = []\n",
    "    for i in range(num_processes):\n",
    "        p = mp.Process(target=run_env, args=(env_name, num_episodes, i))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e3797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\wreep\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "EPISODES = 1000                # Total training episodes\n",
    "BATCH_SIZE = 32                # Batch size for training\n",
    "MEMORY_SIZE = 100000           # Larger replay memory (for PER)\n",
    "GAMMA = 0.99                   # Higher discount factor\n",
    "EPSILON_START = 1.0            # Starting exploration rate\n",
    "EPSILON_MIN = 0.01             # Min exploration rate\n",
    "EPSILON_DECAY = 0.995          # Decay rate\n",
    "LEARNING_RATE = 0.00025        # Lower learning rate (better stability)\n",
    "UPDATE_TARGET_FREQ = 1000      # Double DQN target network update\n",
    "FRAME_STACK = 4                # Number of stacked frames (temporal info)\n",
    "SAVE_FREQ = 50                 # Save weights every N episodes\n",
    "NUM_ENVS = 4                   # Number of parallel environments\n",
    "\n",
    "# --- Prioritized Experience Replay (PER) ---\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.priorities = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        max_priority = max(self.priorities) if self.buffer else 1.0\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(max_priority)\n",
    "    \n",
    "    def sample(self, batch_size, alpha=0.6):\n",
    "        priorities = np.array(self.priorities)\n",
    "        probs = priorities ** alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-0.4)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        states = np.array([x[0] for x in samples])\n",
    "        actions = np.array([x[1] for x in samples])\n",
    "        rewards = np.array([x[2] for x in samples])\n",
    "        next_states = np.array([x[3] for x in samples])\n",
    "        dones = np.array([x[4] for x in samples])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, errors, offset=0.01):\n",
    "        for i, error in zip(indices, errors):\n",
    "            self.priorities[i] = abs(error) + offset\n",
    "\n",
    "# --- Environment Worker ---\n",
    "class EnvironmentWorker(mp.Process):\n",
    "    def __init__(self, task_queue, result_queue, model_weights, epsilon):\n",
    "        super().__init__()\n",
    "        self.task_queue = task_queue\n",
    "        self.result_queue = result_queue\n",
    "        self.model_weights = model_weights\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def run(self):\n",
    "        # Create local environment and model\n",
    "        env = gym.make(\"ALE/Frogger-v5\", render_mode=\"rgb_array\")\n",
    "        model = build_model()\n",
    "        model.set_weights(self.model_weights)\n",
    "        frame_stacker = FrameStacker()\n",
    "        \n",
    "        while True:\n",
    "            task = self.task_queue.get()\n",
    "            if task is None:  # Termination signal\n",
    "                env.close()\n",
    "                break\n",
    "                \n",
    "            # Get initial state\n",
    "            state, _ = env.reset()\n",
    "            state = preprocess_state(state)\n",
    "            state = frame_stacker.reset(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                # Epsilon-greedy action\n",
    "                if np.random.rand() <= self.epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "                    action = np.argmax(q_values[0])\n",
    "                \n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                next_state = preprocess_state(next_state)\n",
    "                next_state = frame_stacker.append(next_state)\n",
    "                total_reward += reward\n",
    "                \n",
    "                # Send experience to main process\n",
    "                self.result_queue.put((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "            \n",
    "            # Send episode results\n",
    "            self.result_queue.put(('episode_result', total_reward))\n",
    "\n",
    "# --- Model Definition ---\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (8, 8), strides=4, activation=\"relu\", input_shape=(84, 84, FRAME_STACK)),\n",
    "        Conv2D(64, (4, 4), strides=2, activation=\"relu\"),\n",
    "        Conv2D(64, (3, 3), strides=1, activation=\"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dense(env.action_space.n, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "    return model\n",
    "\n",
    "# --- Frame Stacking ---\n",
    "class FrameStacker:\n",
    "    def __init__(self):\n",
    "        self.frames = deque(maxlen=FRAME_STACK)\n",
    "    \n",
    "    def reset(self, state):\n",
    "        for _ in range(FRAME_STACK):\n",
    "            self.frames.append(state)\n",
    "        return np.stack(self.frames, axis=-1)\n",
    "    \n",
    "    def append(self, state):\n",
    "        self.frames.append(state)\n",
    "        return np.stack(self.frames, axis=-1)\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess_state(state):\n",
    "    state = np.mean(state, axis=2)  # Grayscale\n",
    "    state = state[34:194, :]        # Crop\n",
    "    state = state[::2, ::2]         # Downsample to 80x80\n",
    "    state = np.pad(state, ((2,2),(2,2)), mode='constant')\n",
    "    return state / 255.0\n",
    "\n",
    "# --- Main Training Function ---\n",
    "def train_dqn_parallel():\n",
    "    # Initialize main model and target model\n",
    "    main_model = build_model()\n",
    "    target_model = clone_model(main_model)\n",
    "    target_model.set_weights(main_model.get_weights())\n",
    "    \n",
    "    # Create shared memory for model weights\n",
    "    shared_weights = mp.Array('d', main_model.get_weights()[0].flatten())\n",
    "    \n",
    "    # Create queues and workers\n",
    "    task_queue = mp.Queue()\n",
    "    result_queue = mp.Queue()\n",
    "    workers = []\n",
    "    \n",
    "    for _ in range(NUM_ENVS):\n",
    "        worker = EnvironmentWorker(task_queue, result_queue, main_model.get_weights(), EPSILON_START)\n",
    "        worker.start()\n",
    "        workers.append(worker)\n",
    "    \n",
    "    # Initialize replay memory\n",
    "    memory = PrioritizedReplayBuffer(MEMORY_SIZE)\n",
    "    rewards_history = []\n",
    "    global_step = 0\n",
    "    epsilon = EPSILON_START\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Start episodes\n",
    "    for episode in range(1, EPISODES + 1):\n",
    "        # Send tasks to workers\n",
    "        for _ in range(NUM_ENVS):\n",
    "            task_queue.put(('start_episode',))\n",
    "        \n",
    "        # Collect experiences and results\n",
    "        episode_rewards = []\n",
    "        for _ in range(NUM_ENVS):\n",
    "            while True:\n",
    "                result = result_queue.get()\n",
    "                if result[0] == 'episode_result':\n",
    "                    episode_rewards.append(result[1])\n",
    "                    break\n",
    "                else:\n",
    "                    state, action, reward, next_state, done = result\n",
    "                    memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Train on collected experiences\n",
    "        if len(memory.buffer) >= BATCH_SIZE:\n",
    "            states, actions, rewards, next_states, dones, indices, weights = memory.sample(BATCH_SIZE)\n",
    "            \n",
    "            # Double DQN update\n",
    "            next_q_values = target_model.predict(next_states, verbose=0)\n",
    "            best_actions = np.argmax(main_model.predict(next_states, verbose=0), axis=1)\n",
    "            target_q = rewards + GAMMA * next_q_values[np.arange(BATCH_SIZE), best_actions] * (1 - dones)\n",
    "            \n",
    "            # Update priorities\n",
    "            current_q = main_model.predict(states, verbose=0)\n",
    "            td_errors = target_q - current_q[np.arange(BATCH_SIZE), actions]\n",
    "            memory.update_priorities(indices, td_errors)\n",
    "            \n",
    "            # Train main model\n",
    "            target = current_q.copy()\n",
    "            target[np.arange(BATCH_SIZE), actions] = target_q\n",
    "            main_model.fit(states, target, sample_weight=weights, epochs=1, verbose=0)\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            # Update target network\n",
    "            if global_step % UPDATE_TARGET_FREQ == 0:\n",
    "                target_model.set_weights(main_model.get_weights())\n",
    "        \n",
    "        # Update epsilon\n",
    "        if epsilon > EPSILON_MIN:\n",
    "            epsilon *= EPSILON_DECAY\n",
    "        \n",
    "        # Update workers with new epsilon and weights\n",
    "        for worker in workers:\n",
    "            worker.epsilon = epsilon\n",
    "        \n",
    "        # Update shared weights\n",
    "        main_model.save_weights('temp_weights.h5')\n",
    "        new_weights = main_model.get_weights()\n",
    "        for worker in workers:\n",
    "            worker.model_weights = new_weights\n",
    "        \n",
    "        # Record and print progress\n",
    "        avg_reward = np.mean(episode_rewards)\n",
    "        rewards_history.append(avg_reward)\n",
    "        \n",
    "        elapsed_time = (time.time() - start_time) / 60\n",
    "        remaining_time = (elapsed_time / episode) * (EPISODES - episode)\n",
    "        print(f\"Episode: {episode}/{EPISODES}, Avg Reward: {avg_reward:.2f}, Epsilon: {epsilon:.2f}, Time: {elapsed_time:.2f} mins\")\n",
    "        \n",
    "        # Save weights periodically\n",
    "        if episode % SAVE_FREQ == 0:\n",
    "            main_model.save_weights(f\"frogger_parallel_ep{episode}.weights.h5\")\n",
    "    \n",
    "    # Clean up workers\n",
    "    for _ in range(NUM_ENVS):\n",
    "        task_queue.put(None)\n",
    "    for worker in workers:\n",
    "        worker.join()\n",
    "    \n",
    "    # Plot and save results\n",
    "    plt.plot(rewards_history)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.title(\"Parallel DQN Training Progress\")\n",
    "    plt.savefig(\"frogger_parallel_training.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return main_model\n",
    "\n",
    "# --- Run Training ---\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"ALE/Frogger-v5\")  # For action space reference\n",
    "    model = train_dqn_parallel()\n",
    "    model.save_weights(\"frogger_parallel_final.weights.h5\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ecbbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
