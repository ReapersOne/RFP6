{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!pip install \"numpy<2.0.0\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5351f260",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (2.4.0) or chardet (4.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import envs\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import warnings\n",
    "import ale_py\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3375b7c",
   "metadata": {},
   "source": [
    "# Please experiment with the numbers, we want to make it to the end, and I know it'll take a while so please do not randomly choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e3797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wreep\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/5000, Reward: 11.0, Epsilon: 0.99, Time Elapsed: 0.35 mins, Remaining: 1739.96 mins\n",
      "Episode: 2/5000, Reward: 8.0, Epsilon: 0.99, Time Elapsed: 0.69 mins, Remaining: 1716.45 mins\n",
      "Episode: 3/5000, Reward: 10.0, Epsilon: 0.99, Time Elapsed: 1.03 mins, Remaining: 1717.78 mins\n",
      "Episode: 4/5000, Reward: 8.0, Epsilon: 0.98, Time Elapsed: 1.32 mins, Remaining: 1648.72 mins\n",
      "Episode: 5/5000, Reward: 10.0, Epsilon: 0.98, Time Elapsed: 1.71 mins, Remaining: 1710.22 mins\n",
      "Episode: 6/5000, Reward: 8.0, Epsilon: 0.97, Time Elapsed: 2.05 mins, Remaining: 1707.77 mins\n",
      "Episode: 7/5000, Reward: 11.0, Epsilon: 0.97, Time Elapsed: 2.41 mins, Remaining: 1720.89 mins\n",
      "Episode: 8/5000, Reward: 10.0, Epsilon: 0.96, Time Elapsed: 2.72 mins, Remaining: 1695.07 mins\n",
      "Episode: 9/5000, Reward: 10.0, Epsilon: 0.96, Time Elapsed: 3.08 mins, Remaining: 1710.03 mins\n",
      "Episode: 10/5000, Reward: 12.0, Epsilon: 0.95, Time Elapsed: 3.43 mins, Remaining: 1712.74 mins\n",
      "Episode: 11/5000, Reward: 10.0, Epsilon: 0.95, Time Elapsed: 3.82 mins, Remaining: 1734.52 mins\n",
      "Episode: 12/5000, Reward: 8.0, Epsilon: 0.94, Time Elapsed: 4.22 mins, Remaining: 1752.84 mins\n",
      "Episode: 13/5000, Reward: 8.0, Epsilon: 0.94, Time Elapsed: 4.49 mins, Remaining: 1721.48 mins\n",
      "Episode: 14/5000, Reward: 9.0, Epsilon: 0.93, Time Elapsed: 4.83 mins, Remaining: 1721.31 mins\n",
      "Episode: 15/5000, Reward: 10.0, Epsilon: 0.93, Time Elapsed: 5.20 mins, Remaining: 1727.54 mins\n",
      "Episode: 16/5000, Reward: 16.0, Epsilon: 0.92, Time Elapsed: 5.57 mins, Remaining: 1734.00 mins\n",
      "Episode: 17/5000, Reward: 9.0, Epsilon: 0.92, Time Elapsed: 5.94 mins, Remaining: 1742.09 mins\n",
      "Episode: 18/5000, Reward: 7.0, Epsilon: 0.91, Time Elapsed: 6.22 mins, Remaining: 1722.69 mins\n",
      "Episode: 19/5000, Reward: 7.0, Epsilon: 0.91, Time Elapsed: 6.60 mins, Remaining: 1729.94 mins\n",
      "Episode: 20/5000, Reward: 10.0, Epsilon: 0.90, Time Elapsed: 6.98 mins, Remaining: 1739.22 mins\n",
      "Episode: 21/5000, Reward: 7.0, Epsilon: 0.90, Time Elapsed: 7.33 mins, Remaining: 1736.77 mins\n",
      "Episode: 22/5000, Reward: 11.0, Epsilon: 0.90, Time Elapsed: 7.67 mins, Remaining: 1734.60 mins\n",
      "Episode: 23/5000, Reward: 9.0, Epsilon: 0.89, Time Elapsed: 7.98 mins, Remaining: 1727.45 mins\n",
      "Episode: 24/5000, Reward: 9.0, Epsilon: 0.89, Time Elapsed: 8.36 mins, Remaining: 1732.51 mins\n",
      "Episode: 25/5000, Reward: 15.0, Epsilon: 0.88, Time Elapsed: 8.77 mins, Remaining: 1744.85 mins\n",
      "Episode: 26/5000, Reward: 8.0, Epsilon: 0.88, Time Elapsed: 9.10 mins, Remaining: 1740.06 mins\n",
      "Episode: 27/5000, Reward: 10.0, Epsilon: 0.87, Time Elapsed: 9.41 mins, Remaining: 1733.56 mins\n",
      "Episode: 28/5000, Reward: 15.0, Epsilon: 0.87, Time Elapsed: 9.75 mins, Remaining: 1731.49 mins\n",
      "Episode: 29/5000, Reward: 7.0, Epsilon: 0.86, Time Elapsed: 10.16 mins, Remaining: 1741.96 mins\n",
      "Episode: 30/5000, Reward: 10.0, Epsilon: 0.86, Time Elapsed: 10.52 mins, Remaining: 1742.46 mins\n",
      "Episode: 31/5000, Reward: 12.0, Epsilon: 0.86, Time Elapsed: 10.93 mins, Remaining: 1752.47 mins\n",
      "Episode: 32/5000, Reward: 6.0, Epsilon: 0.85, Time Elapsed: 11.24 mins, Remaining: 1745.56 mins\n",
      "Episode: 33/5000, Reward: 11.0, Epsilon: 0.85, Time Elapsed: 11.62 mins, Remaining: 1749.33 mins\n",
      "Episode: 34/5000, Reward: 8.0, Epsilon: 0.84, Time Elapsed: 11.98 mins, Remaining: 1749.20 mins\n",
      "Episode: 35/5000, Reward: 9.0, Epsilon: 0.84, Time Elapsed: 12.48 mins, Remaining: 1770.57 mins\n",
      "Episode: 36/5000, Reward: 11.0, Epsilon: 0.83, Time Elapsed: 12.87 mins, Remaining: 1775.04 mins\n",
      "Episode: 37/5000, Reward: 9.0, Epsilon: 0.83, Time Elapsed: 13.29 mins, Remaining: 1783.14 mins\n",
      "Episode: 38/5000, Reward: 13.0, Epsilon: 0.83, Time Elapsed: 13.73 mins, Remaining: 1792.87 mins\n",
      "Episode: 39/5000, Reward: 12.0, Epsilon: 0.82, Time Elapsed: 14.12 mins, Remaining: 1796.16 mins\n",
      "Episode: 40/5000, Reward: 9.0, Epsilon: 0.82, Time Elapsed: 14.57 mins, Remaining: 1806.59 mins\n",
      "Episode: 41/5000, Reward: 14.0, Epsilon: 0.81, Time Elapsed: 14.95 mins, Remaining: 1808.41 mins\n",
      "Episode: 42/5000, Reward: 10.0, Epsilon: 0.81, Time Elapsed: 15.32 mins, Remaining: 1808.36 mins\n",
      "Episode: 43/5000, Reward: 8.0, Epsilon: 0.81, Time Elapsed: 15.65 mins, Remaining: 1803.89 mins\n",
      "Episode: 44/5000, Reward: 10.0, Epsilon: 0.80, Time Elapsed: 16.01 mins, Remaining: 1803.10 mins\n",
      "Episode: 45/5000, Reward: 8.0, Epsilon: 0.80, Time Elapsed: 16.41 mins, Remaining: 1806.86 mins\n",
      "Episode: 46/5000, Reward: 12.0, Epsilon: 0.79, Time Elapsed: 16.80 mins, Remaining: 1809.60 mins\n",
      "Episode: 47/5000, Reward: 10.0, Epsilon: 0.79, Time Elapsed: 17.16 mins, Remaining: 1807.97 mins\n",
      "Episode: 48/5000, Reward: 8.0, Epsilon: 0.79, Time Elapsed: 17.59 mins, Remaining: 1814.62 mins\n",
      "Episode: 49/5000, Reward: 7.0, Epsilon: 0.78, Time Elapsed: 17.91 mins, Remaining: 1809.66 mins\n",
      "Episode: 50/5000, Reward: 9.0, Epsilon: 0.78, Time Elapsed: 18.28 mins, Remaining: 1809.47 mins\n",
      "Saved weights at episode 50\n",
      "Episode: 51/5000, Reward: 11.0, Epsilon: 0.77, Time Elapsed: 18.96 mins, Remaining: 1839.84 mins\n",
      "Episode: 52/5000, Reward: 10.0, Epsilon: 0.77, Time Elapsed: 19.34 mins, Remaining: 1840.64 mins\n",
      "Episode: 53/5000, Reward: 15.0, Epsilon: 0.77, Time Elapsed: 19.92 mins, Remaining: 1859.59 mins\n",
      "Episode: 54/5000, Reward: 18.0, Epsilon: 0.76, Time Elapsed: 20.40 mins, Remaining: 1868.38 mins\n",
      "Episode: 55/5000, Reward: 11.0, Epsilon: 0.76, Time Elapsed: 20.81 mins, Remaining: 1870.92 mins\n",
      "Episode: 56/5000, Reward: 10.0, Epsilon: 0.76, Time Elapsed: 21.27 mins, Remaining: 1877.90 mins\n",
      "Episode: 57/5000, Reward: 9.0, Epsilon: 0.75, Time Elapsed: 21.71 mins, Remaining: 1882.47 mins\n",
      "Episode: 58/5000, Reward: 10.0, Epsilon: 0.75, Time Elapsed: 22.16 mins, Remaining: 1888.52 mins\n",
      "Episode: 59/5000, Reward: 13.0, Epsilon: 0.74, Time Elapsed: 22.63 mins, Remaining: 1894.84 mins\n",
      "Episode: 60/5000, Reward: 14.0, Epsilon: 0.74, Time Elapsed: 23.01 mins, Remaining: 1894.38 mins\n",
      "Episode: 61/5000, Reward: 10.0, Epsilon: 0.74, Time Elapsed: 23.41 mins, Remaining: 1895.16 mins\n",
      "Episode: 62/5000, Reward: 9.0, Epsilon: 0.73, Time Elapsed: 23.85 mins, Remaining: 1899.37 mins\n",
      "Episode: 63/5000, Reward: 12.0, Epsilon: 0.73, Time Elapsed: 24.21 mins, Remaining: 1897.37 mins\n",
      "Episode: 64/5000, Reward: 11.0, Epsilon: 0.73, Time Elapsed: 24.68 mins, Remaining: 1903.26 mins\n",
      "Episode: 65/5000, Reward: 11.0, Epsilon: 0.72, Time Elapsed: 25.08 mins, Remaining: 1904.11 mins\n",
      "Episode: 66/5000, Reward: 13.0, Epsilon: 0.72, Time Elapsed: 25.57 mins, Remaining: 1911.36 mins\n",
      "Episode: 67/5000, Reward: 8.0, Epsilon: 0.71, Time Elapsed: 25.94 mins, Remaining: 1910.03 mins\n",
      "Episode: 68/5000, Reward: 13.0, Epsilon: 0.71, Time Elapsed: 26.36 mins, Remaining: 1911.80 mins\n",
      "Episode: 69/5000, Reward: 11.0, Epsilon: 0.71, Time Elapsed: 26.76 mins, Remaining: 1912.38 mins\n",
      "Episode: 70/5000, Reward: 14.0, Epsilon: 0.70, Time Elapsed: 27.25 mins, Remaining: 1919.23 mins\n",
      "Episode: 71/5000, Reward: 12.0, Epsilon: 0.70, Time Elapsed: 27.75 mins, Remaining: 1926.30 mins\n",
      "Episode: 72/5000, Reward: 10.0, Epsilon: 0.70, Time Elapsed: 28.22 mins, Remaining: 1931.38 mins\n",
      "Episode: 73/5000, Reward: 14.0, Epsilon: 0.69, Time Elapsed: 28.73 mins, Remaining: 1939.33 mins\n",
      "Episode: 74/5000, Reward: 8.0, Epsilon: 0.69, Time Elapsed: 29.16 mins, Remaining: 1941.20 mins\n",
      "Episode: 75/5000, Reward: 10.0, Epsilon: 0.69, Time Elapsed: 29.63 mins, Remaining: 1945.75 mins\n",
      "Episode: 76/5000, Reward: 11.0, Epsilon: 0.68, Time Elapsed: 30.08 mins, Remaining: 1948.65 mins\n",
      "Episode: 77/5000, Reward: 12.0, Epsilon: 0.68, Time Elapsed: 30.51 mins, Remaining: 1950.44 mins\n",
      "Episode: 78/5000, Reward: 9.0, Epsilon: 0.68, Time Elapsed: 30.95 mins, Remaining: 1953.33 mins\n",
      "Episode: 79/5000, Reward: 15.0, Epsilon: 0.67, Time Elapsed: 31.51 mins, Remaining: 1962.81 mins\n",
      "Episode: 80/5000, Reward: 11.0, Epsilon: 0.67, Time Elapsed: 32.07 mins, Remaining: 1972.14 mins\n",
      "Episode: 81/5000, Reward: 12.0, Epsilon: 0.67, Time Elapsed: 32.54 mins, Remaining: 1976.31 mins\n",
      "Episode: 82/5000, Reward: 7.0, Epsilon: 0.66, Time Elapsed: 33.03 mins, Remaining: 1981.18 mins\n",
      "Episode: 83/5000, Reward: 8.0, Epsilon: 0.66, Time Elapsed: 33.40 mins, Remaining: 1978.91 mins\n",
      "Episode: 84/5000, Reward: 10.0, Epsilon: 0.66, Time Elapsed: 33.89 mins, Remaining: 1983.18 mins\n",
      "Episode: 85/5000, Reward: 10.0, Epsilon: 0.65, Time Elapsed: 34.42 mins, Remaining: 1990.38 mins\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 86/5000, Reward: 9.0, Epsilon: 0.65, Time Elapsed: 34.94 mins, Remaining: 1996.58 mins\n",
      "Episode: 87/5000, Reward: 8.0, Epsilon: 0.65, Time Elapsed: 35.36 mins, Remaining: 1996.67 mins\n",
      "Episode: 88/5000, Reward: 11.0, Epsilon: 0.64, Time Elapsed: 35.89 mins, Remaining: 2003.29 mins\n",
      "Episode: 89/5000, Reward: 15.0, Epsilon: 0.64, Time Elapsed: 36.41 mins, Remaining: 2009.28 mins\n",
      "Episode: 90/5000, Reward: 13.0, Epsilon: 0.64, Time Elapsed: 36.99 mins, Remaining: 2017.83 mins\n",
      "Episode: 91/5000, Reward: 21.0, Epsilon: 0.63, Time Elapsed: 37.63 mins, Remaining: 2030.02 mins\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameters ---\n",
    "EPISODES = 5000                \n",
    "BATCH_SIZE = 32                \n",
    "MEMORY_SIZE = 100000           \n",
    "GAMMA = 0.99                   \n",
    "EPSILON_START = 1.0            \n",
    "EPSILON_MIN = 0.01             \n",
    "EPSILON_DECAY = 0.995          \n",
    "LEARNING_RATE = 0.00025        \n",
    "UPDATE_TARGET_FREQ = 1000      \n",
    "FRAME_STACK = 4                \n",
    "SAVE_FREQ = 50                 \n",
    "\n",
    "# --- Prioritized Experience Replay (PER) ---\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.priorities = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        max_priority = max(self.priorities) if self.buffer else 1.0\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(max_priority)\n",
    "    \n",
    "    def sample(self, batch_size, alpha=0.6):\n",
    "        priorities = np.array(self.priorities)\n",
    "        probs = priorities ** alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-0.4)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        states = np.array([x[0] for x in samples])\n",
    "        actions = np.array([x[1] for x in samples])\n",
    "        rewards = np.array([x[2] for x in samples])\n",
    "        next_states = np.array([x[3] for x in samples])\n",
    "        dones = np.array([x[4] for x in samples])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, errors, offset=0.01):\n",
    "        for i, error in zip(indices, errors):\n",
    "            self.priorities[i] = abs(error) + offset\n",
    "\n",
    "# --- Environment Setup ---\n",
    "env = gym.make(\"ALE/Frogger-v5\", render_mode=\"rgb_array\")\n",
    "state_shape = (84, 84, FRAME_STACK)\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# --- Model Definition ---\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (8, 8), strides=4, activation=\"relu\", input_shape=state_shape),\n",
    "        Conv2D(64, (4, 4), strides=2, activation=\"relu\"),\n",
    "        Conv2D(64, (3, 3), strides=1, activation=\"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dense(action_size, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "    return model\n",
    "\n",
    "# --- Frame Stacking ---\n",
    "class FrameStacker:\n",
    "    def __init__(self):\n",
    "        self.frames = deque(maxlen=FRAME_STACK)\n",
    "    \n",
    "    def reset(self, state):\n",
    "        for _ in range(FRAME_STACK):\n",
    "            self.frames.append(state)\n",
    "        return np.stack(self.frames, axis=-1)\n",
    "    \n",
    "    def append(self, state):\n",
    "        self.frames.append(state)\n",
    "        return np.stack(self.frames, axis=-1)\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess_state(state):\n",
    "    state = np.mean(state, axis=2)  \n",
    "    state = state[34:194, :]        \n",
    "    state = state[::2, ::2]         \n",
    "    state = np.pad(state, ((2,2),(2,2)), mode='constant')\n",
    "    return state / 255.0\n",
    "\n",
    "# --- Training Functions ---\n",
    "def train_dqn():\n",
    "    model = build_model()\n",
    "    target_model = clone_model(model)  \n",
    "    target_model.set_weights(model.get_weights())\n",
    "    \n",
    "    memory = PrioritizedReplayBuffer(MEMORY_SIZE)  \n",
    "    frame_stacker = FrameStacker()\n",
    "    epsilon = EPSILON_START\n",
    "    rewards_history = []\n",
    "    start_time = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for episode in range(1, EPISODES + 1):\n",
    "        state, _ = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "        state = frame_stacker.reset(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            global_step += 1\n",
    "\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "                action = np.argmax(q_values[0])\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = preprocess_state(next_state)\n",
    "            next_state = frame_stacker.append(next_state)\n",
    "            memory.add(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if global_step % 4 == 0 and len(memory.buffer) >= BATCH_SIZE:\n",
    "                states, actions, rewards, next_states, dones, indices, weights = memory.sample(BATCH_SIZE)\n",
    "                \n",
    "                next_q_values = target_model.predict(next_states, verbose=0)\n",
    "                best_actions = np.argmax(model.predict(next_states, verbose=0), axis=1)\n",
    "                target_q = rewards + GAMMA * next_q_values[np.arange(BATCH_SIZE), best_actions] * (1 - dones)\n",
    "                \n",
    "                current_q = model.predict(states, verbose=0)\n",
    "                td_errors = target_q - current_q[np.arange(BATCH_SIZE), actions]\n",
    "                memory.update_priorities(indices, td_errors)\n",
    "                \n",
    "                target = current_q.copy()\n",
    "                target[np.arange(BATCH_SIZE), actions] = target_q\n",
    "                model.fit(states, target, sample_weight=weights, epochs=1, verbose=0)\n",
    "\n",
    "            if global_step % UPDATE_TARGET_FREQ == 0:\n",
    "                target_model.set_weights(model.get_weights())\n",
    "\n",
    "        if epsilon > EPSILON_MIN:\n",
    "            epsilon *= EPSILON_DECAY\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "\n",
    "        elapsed_time = (time.time() - start_time) / 60  # in minutes\n",
    "        remaining_time = (elapsed_time / episode) * (EPISODES - episode)\n",
    "        print(f\"Episode: {episode}/{EPISODES}, Reward: {total_reward}, Epsilon: {epsilon:.2f}, Time Elapsed: {elapsed_time:.2f} mins, Remaining: {remaining_time:.2f} mins\")\n",
    "\n",
    "        if episode % SAVE_FREQ == 0:\n",
    "            model.save_weights(f\"frogger_weights_ep{episode}.weights.h5\")\n",
    "            print(f\"Saved weights at episode {episode}\")\n",
    "\n",
    "    plt.plot(rewards_history)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Frogger DQN Training (PER + Double DQN + Frame Stack)\")\n",
    "    plt.savefig(\"frogger_training_enhanced.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Run Training ---\n",
    "if __name__ == \"__main__\":\n",
    "    model = train_dqn()\n",
    "    model.save_weights(\"frogger_final_weights_enhanced.weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
