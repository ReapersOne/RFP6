{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8677f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5351f260",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import envs\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import warnings\n",
    "import ale_py\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e3797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\wreep\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/1000, Reward: 9.0, Epsilon: 0.99, Time Elapsed: 0.29 mins, Remaining: 293.69 mins\n",
      "Episode: 2/1000, Reward: 11.0, Epsilon: 0.99, Time Elapsed: 0.71 mins, Remaining: 353.20 mins\n",
      "Episode: 3/1000, Reward: 9.0, Epsilon: 0.99, Time Elapsed: 1.07 mins, Remaining: 354.50 mins\n",
      "Episode: 4/1000, Reward: 13.0, Epsilon: 0.98, Time Elapsed: 1.39 mins, Remaining: 344.91 mins\n",
      "Episode: 5/1000, Reward: 11.0, Epsilon: 0.98, Time Elapsed: 1.71 mins, Remaining: 341.06 mins\n",
      "Episode: 6/1000, Reward: 10.0, Epsilon: 0.97, Time Elapsed: 2.02 mins, Remaining: 334.33 mins\n",
      "Episode: 7/1000, Reward: 8.0, Epsilon: 0.97, Time Elapsed: 2.28 mins, Remaining: 324.08 mins\n",
      "Episode: 8/1000, Reward: 9.0, Epsilon: 0.96, Time Elapsed: 2.55 mins, Remaining: 316.68 mins\n",
      "Episode: 9/1000, Reward: 9.0, Epsilon: 0.96, Time Elapsed: 2.91 mins, Remaining: 320.32 mins\n",
      "Episode: 10/1000, Reward: 7.0, Epsilon: 0.95, Time Elapsed: 3.20 mins, Remaining: 316.96 mins\n",
      "Episode: 11/1000, Reward: 9.0, Epsilon: 0.95, Time Elapsed: 3.51 mins, Remaining: 315.69 mins\n",
      "Episode: 12/1000, Reward: 7.0, Epsilon: 0.94, Time Elapsed: 3.87 mins, Remaining: 318.91 mins\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameters ---\n",
    "EPISODES = 1000                \n",
    "BATCH_SIZE = 32                \n",
    "MEMORY_SIZE = 100000           \n",
    "GAMMA = 0.99                   \n",
    "EPSILON_START = 1.0            \n",
    "EPSILON_MIN = 0.01             \n",
    "EPSILON_DECAY = 0.995          \n",
    "LEARNING_RATE = 0.00025        \n",
    "UPDATE_TARGET_FREQ = 1000      \n",
    "FRAME_STACK = 4                \n",
    "SAVE_FREQ = 50                 \n",
    "\n",
    "# --- Prioritized Experience Replay (PER) ---\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.priorities = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        max_priority = max(self.priorities) if self.buffer else 1.0\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(max_priority)\n",
    "    \n",
    "    def sample(self, batch_size, alpha=0.6):\n",
    "        priorities = np.array(self.priorities)\n",
    "        probs = priorities ** alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-0.4)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        states = np.array([x[0] for x in samples])\n",
    "        actions = np.array([x[1] for x in samples])\n",
    "        rewards = np.array([x[2] for x in samples])\n",
    "        next_states = np.array([x[3] for x in samples])\n",
    "        dones = np.array([x[4] for x in samples])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, errors, offset=0.01):\n",
    "        for i, error in zip(indices, errors):\n",
    "            self.priorities[i] = abs(error) + offset\n",
    "\n",
    "# --- Environment Setup ---\n",
    "env = gym.make(\"ALE/Frogger-v5\", render_mode=\"rgb_array\")\n",
    "state_shape = (84, 84, FRAME_STACK)\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# --- Model Definition ---\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (8, 8), strides=4, activation=\"relu\", input_shape=state_shape),\n",
    "        Conv2D(64, (4, 4), strides=2, activation=\"relu\"),\n",
    "        Conv2D(64, (3, 3), strides=1, activation=\"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dense(action_size, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "    return model\n",
    "\n",
    "# --- Frame Stacking ---\n",
    "class FrameStacker:\n",
    "    def __init__(self):\n",
    "        self.frames = deque(maxlen=FRAME_STACK)\n",
    "    \n",
    "    def reset(self, state):\n",
    "        for _ in range(FRAME_STACK):\n",
    "            self.frames.append(state)\n",
    "        return np.stack(self.frames, axis=-1)\n",
    "    \n",
    "    def append(self, state):\n",
    "        self.frames.append(state)\n",
    "        return np.stack(self.frames, axis=-1)\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess_state(state):\n",
    "    state = np.mean(state, axis=2)  \n",
    "    state = state[34:194, :]        \n",
    "    state = state[::2, ::2]         \n",
    "    state = np.pad(state, ((2,2),(2,2)), mode='constant')\n",
    "    return state / 255.0\n",
    "\n",
    "# --- Training Functions ---\n",
    "def train_dqn():\n",
    "    model = build_model()\n",
    "    target_model = clone_model(model)  \n",
    "    target_model.set_weights(model.get_weights())\n",
    "    \n",
    "    memory = PrioritizedReplayBuffer(MEMORY_SIZE)  \n",
    "    frame_stacker = FrameStacker()\n",
    "    epsilon = EPSILON_START\n",
    "    rewards_history = []\n",
    "    start_time = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for episode in range(1, EPISODES + 1):\n",
    "        state, _ = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "        state = frame_stacker.reset(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            global_step += 1\n",
    "\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "                action = np.argmax(q_values[0])\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = preprocess_state(next_state)\n",
    "            next_state = frame_stacker.append(next_state)\n",
    "            memory.add(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if global_step % 4 == 0 and len(memory.buffer) >= BATCH_SIZE:\n",
    "                states, actions, rewards, next_states, dones, indices, weights = memory.sample(BATCH_SIZE)\n",
    "                \n",
    "                next_q_values = target_model.predict(next_states, verbose=0)\n",
    "                best_actions = np.argmax(model.predict(next_states, verbose=0), axis=1)\n",
    "                target_q = rewards + GAMMA * next_q_values[np.arange(BATCH_SIZE), best_actions] * (1 - dones)\n",
    "                \n",
    "                current_q = model.predict(states, verbose=0)\n",
    "                td_errors = target_q - current_q[np.arange(BATCH_SIZE), actions]\n",
    "                memory.update_priorities(indices, td_errors)\n",
    "                \n",
    "                target = current_q.copy()\n",
    "                target[np.arange(BATCH_SIZE), actions] = target_q\n",
    "                model.fit(states, target, sample_weight=weights, epochs=1, verbose=0)\n",
    "\n",
    "            if global_step % UPDATE_TARGET_FREQ == 0:\n",
    "                target_model.set_weights(model.get_weights())\n",
    "\n",
    "        if epsilon > EPSILON_MIN:\n",
    "            epsilon *= EPSILON_DECAY\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "\n",
    "        elapsed_time = (time.time() - start_time) / 60  # in minutes\n",
    "        remaining_time = (elapsed_time / episode) * (EPISODES - episode)\n",
    "        print(f\"Episode: {episode}/{EPISODES}, Reward: {total_reward}, Epsilon: {epsilon:.2f}, Time Elapsed: {elapsed_time:.2f} mins, Remaining: {remaining_time:.2f} mins\")\n",
    "\n",
    "        if episode % SAVE_FREQ == 0:\n",
    "            model.save_weights(f\"frogger_weights_ep{episode}.weights.h5\")\n",
    "            print(f\"Saved weights at episode {episode}\")\n",
    "\n",
    "    plt.plot(rewards_history)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Frogger DQN Training (PER + Double DQN + Frame Stack)\")\n",
    "    plt.savefig(\"frogger_training_enhanced.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Run Training ---\n",
    "if __name__ == \"__main__\":\n",
    "    model = train_dqn()\n",
    "    model.save_weights(\"frogger_final_weights_enhanced.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84dbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
