{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "957fa03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import envs\n",
    "\n",
    "# List all registered environments (including Atari)\n",
    "all_envs = envs.registry.keys()\n",
    "atari_envs = [env_id for env_id in all_envs if \"ALE/\" in env_id]\n",
    "\n",
    "# Print first 10 Atari games\n",
    "print(list(atari_envs)[:10])  # Should include 'ALE/Frogger-v5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5351f260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wreep\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\wreep\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72332e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport gymnasium as gym\\n\\n# Modern way to create Atari environment\\nenv = gym.make(\"ALE/Frogger-v5\", render_mode=\"human\")  # Note: ALE/ prefix required\\n\\n\\nobservation, info = env.reset()\\nfor _ in range(100):\\n    action = env.action_space.sample()  # Random action\\n    observation, reward, terminated, truncated, info = env.step(action)\\n        \\n    if terminated or truncated:\\n        observation, info = env.reset()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import gymnasium as gym\n",
    "\n",
    "# Modern way to create Atari environment\n",
    "env = gym.make(\"ALE/Frogger-v5\", render_mode=\"human\")  # Note: ALE/ prefix required\n",
    "\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd2e3797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Success Rate: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1380\\3681929919.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqtable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Best Q-value action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscretize_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_space_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mterminated\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\wrappers\\common.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m     def reset(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    320\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[0;32m    321\u001b[0m         \u001b[1;34m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     def reset(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\wrappers\\common.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     def reset(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\ale_py\\env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mis_terminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_truncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make(\"ALE/Frogger-v5\", render_mode=\"rgb_array\", obs_type=\"grayscale\")  # Simplified observation\n",
    "\n",
    "def discretize_observation(obs, bins):\n",
    "    \"\"\"Process 210x160 grayscale image into simplified state\"\"\"\n",
    "    # Find frog position (bright pixel in bottom area)\n",
    "    frog_area = obs[180:200, 70:90]  # Approximate frog spawn area\n",
    "    frog_y, frog_x = np.unravel_index(np.argmax(frog_area), frog_area.shape)\n",
    "    \n",
    "    # Find closest lane (horizontal line detection)\n",
    "    lanes = [50, 75, 100, 125]  # Approximate lane positions\n",
    "    current_lane = np.argmin([abs(frog_y - lane) for lane in lanes])\n",
    "    \n",
    "    # Discretize\n",
    "    y_bin = min(int(frog_y * bins[0] / 210), bins[0]-1)\n",
    "    lane_bin = min(current_lane, bins[1]-1)\n",
    "    return (y_bin, lane_bin)\n",
    "\n",
    "# Q-table setup\n",
    "obs_space_size = (10, 4)  # (y-position bins, lane bins)\n",
    "action_space_size = env.action_space.n\n",
    "qtable = np.zeros((*obs_space_size, action_space_size))\n",
    "\n",
    "# Rest of your Q-learning code...\n",
    "# Q-table setup\n",
    "obs_space_size = (10, 10)  # Simplified state space\n",
    "action_space_size = env.action_space.n\n",
    "qtable = np.zeros((*obs_space_size, action_space_size))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "# Tracking variables\n",
    "total_episodes = 1000\n",
    "successful_episodes = 0\n",
    "total_steps = 0\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state = discretize_observation(env.reset()[0], obs_space_size)\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Random action\n",
    "        else:\n",
    "            action = np.argmax(qtable[state])  # Best Q-value action\n",
    "\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        new_state = discretize_observation(new_state, obs_space_size)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Q-table update\n",
    "        qtable[state][action] = (1 - alpha) * qtable[state][action] + alpha * (\n",
    "            reward + gamma * np.max(qtable[new_state]))\n",
    "        \n",
    "        state = new_state  # This line was likely misaligned in your original code\n",
    "        steps += 1\n",
    "\n",
    "    total_steps += steps\n",
    "    if reward > 0:  # Positive reward for successful crossing\n",
    "        successful_episodes += 1\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {episode}, Success Rate: {successful_episodes/(episode+1):.2f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Results\n",
    "success_rate = successful_episodes / total_episodes\n",
    "print(f\"\\nFinal Success Rate: {success_rate * 100:.2f}%\")\n",
    "print(f\"Average Steps per Episode: {total_steps / total_episodes:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46979e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb17d85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
