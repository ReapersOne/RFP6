{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc93238a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import envs\n",
    "\n",
    "# List all registered environments (including Atari)\n",
    "all_envs = envs.registry.keys()\n",
    "atari_envs = [env_id for env_id in all_envs if \"ALE/\" in env_id]\n",
    "\n",
    "# Print first 10 Atari games\n",
    "print(list(atari_envs)[:10])  # Should include 'ALE/Frogger-v5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5351f260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wreep\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\wreep\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72332e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport gymnasium as gym\\n\\n# Modern way to create Atari environment\\nenv = gym.make(\"ALE/Frogger-v5\", render_mode=\"human\")  # Note: ALE/ prefix required\\n\\n\\nobservation, info = env.reset()\\nfor _ in range(100):\\n    action = env.action_space.sample()  # Random action\\n    observation, reward, terminated, truncated, info = env.step(action)\\n        \\n    if terminated or truncated:\\n        observation, info = env.reset()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import gymnasium as gym\n",
    "\n",
    "# Modern way to create Atari environment\n",
    "env = gym.make(\"ALE/Frogger-v5\", render_mode=\"human\")  # Note: ALE/ prefix required\n",
    "\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02eadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Success Rate: 1.00\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make(\"ALE/Frogger-v5\", render_mode=\"human\", obs_type=\"grayscale\")  # Simplified observation\n",
    "\n",
    "def discretize_observation(obs, bins):\n",
    "    \"\"\"Process 210x160 grayscale image into simplified state\"\"\"\n",
    "    # Find frog position (bright pixel in bottom area)\n",
    "    frog_area = obs[180:200, 70:90]  # Approximate frog spawn area\n",
    "    frog_y, frog_x = np.unravel_index(np.argmax(frog_area), frog_area.shape)\n",
    "    \n",
    "    # Find closest lane (horizontal line detection)\n",
    "    lanes = [50, 75, 100, 125]  # Approximate lane positions\n",
    "    current_lane = np.argmin([abs(frog_y - lane) for lane in lanes])\n",
    "    \n",
    "    # Discretize\n",
    "    y_bin = min(int(frog_y * bins[0] / 210), bins[0]-1)\n",
    "    lane_bin = min(current_lane, bins[1]-1)\n",
    "    return (y_bin, lane_bin)\n",
    "\n",
    "# Q-table setup\n",
    "obs_space_size = (10, 4)  # (y-position bins, lane bins)\n",
    "action_space_size = env.action_space.n\n",
    "qtable = np.zeros((*obs_space_size, action_space_size))\n",
    "\n",
    "# Rest of your Q-learning code...\n",
    "# Q-table setup\n",
    "obs_space_size = (10, 10)  # Simplified state space\n",
    "action_space_size = env.action_space.n\n",
    "qtable = np.zeros((*obs_space_size, action_space_size))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "# Tracking variables\n",
    "total_episodes = 1000\n",
    "successful_episodes = 0\n",
    "total_steps = 0\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state = discretize_observation(env.reset()[0], obs_space_size)\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Random action\n",
    "        else:\n",
    "            action = np.argmax(qtable[state])  # Best Q-value action\n",
    "\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        new_state = discretize_observation(new_state, obs_space_size)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Q-table update\n",
    "        qtable[state][action] = (1 - alpha) * qtable[state][action] + alpha * (\n",
    "            reward + gamma * np.max(qtable[new_state]))\n",
    "        \n",
    "        state = new_state  # This line was likely misaligned in your original code\n",
    "        steps += 1\n",
    "\n",
    "    total_steps += steps\n",
    "    if reward > 0:  # Positive reward for successful crossing\n",
    "        successful_episodes += 1\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {episode}, Success Rate: {successful_episodes/(episode+1):.2f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Results\n",
    "success_rate = successful_episodes / total_episodes\n",
    "print(f\"\\nFinal Success Rate: {success_rate * 100:.2f}%\")\n",
    "print(f\"Average Steps per Episode: {total_steps / total_episodes:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218eb303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
